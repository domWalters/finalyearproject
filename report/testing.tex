\section{Initial Testing}
\subsection{The Data - What to use?}
In the construction of the algorithm and the associated ``Simulation Game'' I'm going to require some toy data that I can use for some simple tests to ascertain whether the algorithm I have written is functional. I obtained a small set of data from within the last decade for companies on the US stock exchange. This data was provided by Shan He, and only consists of a dozen companies. \newline

When it comes to actually putting test data through the algorithm, I will need far more data and will need to take precautions to ensure that the data set isn't ``biased'' in some way (e.g. 80\% of the companies are from the tech sector, and the remaining 20\% are a diverse selection of companies). However biasing isn't inherently bad; if the data set is comprised entirely of tech sector (NASDAQ) stocks then the algorithm will be able to specialise to this particular search space. It's rather likely that companies in the same sector are going to succeed based on somewhat related criteria. \newline

The expanded data set is left until after preliminary testing to be obtained.

\subsection{The Data - Problems}
Upon obtaining the toy data set, I immediately noticed a series of issues:
\begin{itemize}
    \item \bf The data is in .csv format \rm - This potentially isn't an issue, as long as the .csv libraries in Rust are sufficiently quick at reading the files. If this becomes a problem, these files can be translated into a form that is quicker to read (a standard text-file or raw byte data).
    \item \bf Data on each company starts at varying times \rm - This is to be expected; not every company enters the public stock exchange at the same time. Similarly, companies may close or leave the public exchange, or for some reason data for a specific quarter may be unavailable. My solution here is to only run the algorithm over a continuous time period.
    \item \bf Each company provides differing levels of detail \rm - Some companies provide more fields than others. The algorithm won't be able to create a screener that uses data that certain companies don't provide, that wouldn't make any sense. My initial solution is to only allow entries which every company in the data set provides; however, this could be overly limiting.
    \item \bf The documents aren't ordered with regards to header \rm - Each CSV orders it's header in a seemingly arbitrary fashion. This is a problem, as my algorithm assumes the test data it receives is indexed in the same order (i.e the field adj\_price is always in the same position in a row across all .csv files). My solution is to sort the fields into alphabetical order to guarantee alignment. This could potentially be rather cpu intensive as it amounts to rewriting the whole file anew.
\end{itemize}

\subsection{The Data - Implementing the CSV reader} \label{DataCSVRead}
I quickly realised that implementing the requirements I have for this .csv reader is a highly non-trivial task. Specifically, to guarantee that each file has the same header set I will need to iterate over the whole collection of files and every header field in each file, to create a list of shared headers. Sorting this list would then be rather simple (tantamount to a sort of a several hundred element set), but then the files would need to be reassembled in this new order, a task that requires each file to be iterated through as many times as there are elements in the header (although significant optimisations would be possible such as preventing access to indices that have already been used transferred to the new file). \newline

It's not viable to run this every time the algorithm is run; the impact on run-time would be horrendous. Therefore, I elected to write an entirely separate program that fixes all of the issues I listed, and creates a new set of .csv files. This means that the files can be fixed once, and then the new set of files can be indefinitely reused at no further cost to system performance, by following a set of assumptions:
\begin{itemize}
    \item \bf Every file has the same header, in the same order (alphanumerical descending). \rm
    \item \bf The data is row continuous by date and quarter (i.e. there are no missing quarters). \rm
\end{itemize}

\subsection{Functional Tests - What to check?}
The first thing I need to ascertain is whether the .csv reader, genetic algorithm, and simulation game are functionally correct. This can be split into unit tests on each section of the system:
\begin{itemize}
    \item \bf .csv reformatting \rm - The independent program to reformat the .csv test data is ``correct'', and provides files that obeys the set of assumptions presented in \ref{DataCSVRead}.
    \item \bf .csv data loading \rm - The loader that takes the .csv files and parses them in to the program does so ``correctly'', retaining the assumptions in \ref{DataCSVRead}.
    \item \bf Initialisation \rm - The Game initialisation function provides a valid initial game state.
    \item \bf Simulation \rm - The Simulation game provides stocks that are valid to purchase (Note: No assurance that the Simulation doesn't miss valid stocks, as their absence will not meaningfully damage the strategy during optimisation).
    \item \bf Termination \rm - The Game terminates generation iteration after the set number of generations.
    \item \bf Improvement \rm - The algorithm provides a population of solutions that, on average, ``improve'' up to some generation number after which it stays within a sufficiently small neighbourhood of a convergence point.
\end{itemize}

\subsection{Functional Tests - Results}

\subsection{Unforeseen Consequences - Match Percentage} \label{testingConsequences}
In my initial implementation the algorithm required \bf every \rm element of a prospective stock to purchase to be greater than the corresponding elements of the strategy it was being purchased for. However, this is extremely unlikely to happen (close to an impossibility), and as such was causing my algorithm to decide to never purchase any stocks at all. The reason for this is that some of the fields that the algorithm is attempting to optimise simply don't have any relationship to the value of the stock; no matter what the field's value is, the price of the stock doesn't change. \newline

To some degree this is obvious; not every piece of data on a company is going to have a correlation to the companies stock price. The algorithm needs a way to deal with this, by ignoring fields that it deems irrelevant. \newline

Through trial and error I decided to see what would happen with a 50\% match between a prospective stock and a strategy, instead of a 100\%. The algorithm ran, it bought stocks, and it's payoff increased over time. Now we can do some analysis on the final generation of the algorithm, and see which fields each strategy matches on most often. Doing this for the whole population will give us an idea as to which data points are being considered relevant by the algorithm. This can be done by simply counting which fields match for each purchased stock and then incrementing a value in the corresponding index of some tracking vector. \newline

The result of printing this vector is that fields the algorithm ``likes'' have very high numbers, and fields it ``hates'' are 0 (on a long enough timescale). There will also be fields in between. This gave me the idea that I could run the algorithm twice: once to do this analysis and decide which fields are meaningless, and then a second time over the remaining fields but this time requiring a much higher match percentage. This could even be indefinitely repeated until no fields have a 0 result in the preliminary run of the algorithm. \newline

\subsection{Expanded Data Set}
The expanded data set is being obtained from the Intrinio API \cite{intrinioApi}. Intrinio is a company that provides access to stock market information, both current and historical, on a free and premium account basis. I am using a premium account owned by Shan He, and an unofficial Python SDK for the V1.0 of this API \cite{intrinioPython}. \newline

The account allows 5000 calls to the API each day, which allows me to pull .csv files containing the data on around 40-50 companies. My intention is to do this every day for an extended period of time, then each day I put all of the files I've collected so far through my .csv formatter, and then make these files available to all of the other students doing projects under the supervision of Shan He. \newline

In this way, whilst I am hitting the account limit every day, I am not preventing other students accessing the data it provides. \newline
